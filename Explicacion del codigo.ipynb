{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código en Python, ejecutado en un entorno Jupyter Notebook, realiza el análisis de un conjunto de datos clínicos almacenado en un archivo CSV llamado `dataset_elpino.csv`. Aquí te explico paso a paso lo que hace el código:\n",
    "\n",
    "**1. Importación de librerías:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8487c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e47ac",
   "metadata": {},
   "source": [
    "- Importa la librería `pandas` para la manipulación y análisis de datos tabulares.\n",
    "- Importa `matplotlib.pyplot` para la creación de visualizaciones.\n",
    "\n",
    "**2. Lectura y procesamiento del archivo CSV:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4237e021",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_elpino.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m sexo=[]\n\u001b[32m      5\u001b[39m severidad=[]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m archivo=\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_elpino.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m header=archivo.readline().strip().split(\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m features=[]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dataset_elpino.csv'"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "target=[]\n",
    "edad=[]\n",
    "sexo=[]\n",
    "severidad=[]\n",
    "archivo=open(\"dataset_elpino.csv\",encoding=\"utf-8\")\n",
    "header=archivo.readline().strip().split(\";\")\n",
    "features=[]\n",
    "for col in header:\n",
    "    col=col.split(\"-\")[0].strip()\n",
    "    if col.startswith(\"Diag\") or col.startswith(\"Proc\"):\n",
    "        col=col.split(\" \")\n",
    "        col=col[0]+col[1]\n",
    "    features.append(col)\n",
    "for linea in archivo:\n",
    "    row=[]\n",
    "    linea=linea.strip().split(\";\")\n",
    "    for i in range(len(linea)):\n",
    "        col=linea[i].split(\"-\")[0].strip()\n",
    "        if i==67:\n",
    "            grd=col\n",
    "            #target.append(grd[-1])\n",
    "            target.append(grd)\n",
    "        elif i==66:\n",
    "            sexo.append(1 if col==\"Mujer\" else 0)\n",
    "        elif i==65:\n",
    "            edad.append(int(col))\n",
    "        else:\n",
    "            row.append(col)\n",
    "    corpus.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4d6f4",
   "metadata": {},
   "source": [
    "- Inicializa listas vacías para almacenar los datos del corpus, la variable objetivo (target), la edad, el sexo y la severidad.\n",
    "- Abre el archivo `dataset_elpino.csv` en modo lectura, especificando la codificación UTF-8 para manejar caracteres especiales.\n",
    "- Lee la primera línea del archivo, que contiene los nombres de las columnas, y la divide en una lista llamada `header` utilizando el punto y coma (`;`) como separador.\n",
    "- Itera sobre los nombres de las columnas en `header` para limpiar y procesar cada nombre. Elimina la parte después del guion (`-`) y los espacios en blanco. Si el nombre de la columna comienza con \"Diag\" o \"Proc\", lo divide en palabras y las concatena.\n",
    "- Itera sobre cada línea del archivo CSV. Divide cada línea en una lista de valores utilizando el punto y coma como separador.\n",
    "- Procesa cada valor en la línea. Si el índice `i` es 67, extrae el valor de la variable objetivo (GRD) y lo agrega a la lista `target`. Si `i` es 66, extrae el sexo y lo agrega a la lista `sexo` (1 para \"Mujer\", 0 para otros). Si `i` es 65, extrae la edad y la agrega a la lista `edad`. De lo contrario, agrega el valor a la lista `row`.\n",
    "- Agrega la lista `row` al `corpus`.\n",
    "\n",
    "**3. Creación del DataFrame:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7085c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(corpus,columns=features[:-3])\n",
    "df[\"GRD\"]=target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573fc05",
   "metadata": {},
   "source": [
    "- Crea un DataFrame de pandas llamado `df` utilizando los datos del `corpus` y los nombres de las columnas almacenados en `features` (excepto las últimas tres columnas).\n",
    "- Agrega una nueva columna llamada \"GRD\" al DataFrame `df` y le asigna los valores de la lista `target`.\n",
    "\n",
    "**4. Exploración de Datos:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b581502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491e687",
   "metadata": {},
   "source": [
    "- Imprime un resumen del DataFrame `df`, incluyendo el tipo de datos de cada columna y la cantidad de valores no nulos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccded460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"GRD\"].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41593896",
   "metadata": {},
   "source": [
    "- Calcula la frecuencia de cada valor único en la columna \"GRD\" y muestra las 20 categorías más frecuentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d954e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"GRD\"].value_counts().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f197b",
   "metadata": {},
   "source": [
    "- Genera un gráfico de barras de las frecuencias de cada valor en la columna \"GRD\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"GRD\"].value_counts()[:20].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce068b66",
   "metadata": {},
   "source": [
    "- Genera un gráfico de barras de las frecuencias de los 20 valores más frecuentes en la columna \"GRD\".\n",
    "\n",
    "**5. Filtrado de Datos:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro=(df[\"GRD\"]=='146101') | (df[\"GRD\"]=='146102') | (df[\"GRD\"]=='146103')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1468a",
   "metadata": {},
   "source": [
    "- Crea una máscara booleana llamada `filtro` que indica las filas donde la columna \"GRD\" tiene uno de los siguientes valores: '146101', '146102' o '146103'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[filtro]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f3649e",
   "metadata": {},
   "source": [
    "- Muestra las filas del DataFrame `df` que cumplen con la condición especificada en la máscara `filtro`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[filtro][\"GRD\"],df[\"Diag01\"],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16154c7d",
   "metadata": {},
   "source": [
    "- Crea una tabla de contingencia (crosstab) que muestra la frecuencia de cada combinación de valores en las columnas \"GRD\" y \"Diag01\" para las filas que cumplen con la condición especificada en la máscara `filtro`. Incluye márgenes para mostrar los totales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df[filtro]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b4a62",
   "metadata": {},
   "source": [
    "- Crea un nuevo DataFrame llamado `data` que contiene solo las filas del DataFrame `df` que cumplen con la condición especificada en la máscara `filtro`.\n",
    "\n",
    "**6. Análisis de Diagnósticos y Procedimientos:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_procedimientos=[]\n",
    "features_diagnosticos=[]\n",
    "for j in range(1,36):\n",
    "    field=\"Diag\"+str(j).zfill(2)\n",
    "    features_diagnosticos.append(field)\n",
    "for j in range(1,31):\n",
    "    field=\"Proced\"+str(j).zfill(2)\n",
    "    features_procedimientos.append(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4585021",
   "metadata": {},
   "source": [
    "- Inicializa listas vacías para almacenar los nombres de las columnas de procedimientos y diagnósticos.\n",
    "- Itera sobre los números del 1 al 35 y crea los nombres de las columnas de diagnóstico (\"Diag01\", \"Diag02\", ..., \"Diag35\").\n",
    "- Itera sobre los números del 1 al 30 y crea los nombres de las columnas de procedimientos (\"Proced01\", \"Proced02\", ..., \"Proced30\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedimientos={}\n",
    "diagnosticos={}\n",
    "for i, row in data.iterrows():\n",
    "    for j in range(1,36):\n",
    "        field=\"Diag\"+str(j).zfill(2)\n",
    "        if row[field] not in diagnosticos:\n",
    "            diagnosticos[row[field]]=1\n",
    "        else:\n",
    "            diagnosticos[row[field]]+=1\n",
    "    for j in range(1,31):\n",
    "        field=\"Proced\"+str(j).zfill(2)\n",
    "        if row[field] not in procedimientos:\n",
    "            procedimientos[row[field]]=1\n",
    "        else:\n",
    "            procedimientos[row[field]]+=1\n",
    "token_diagnosticos=list(diagnosticos.keys())\n",
    "token_procedimientos=list(procedimientos.keys())\n",
    "vocabulario=len(token_diagnosticos)+len(token_procedimientos)+1\n",
    "print(\"Diagnósticos: \",len(token_diagnosticos))\n",
    "print(\"Procedimientos: \",len(token_procedimientos))\n",
    "print(\"Vocabulario: \",vocabulario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571bf96",
   "metadata": {},
   "source": [
    "- Inicializa diccionarios vacíos para almacenar la frecuencia de cada diagnóstico y procedimiento.\n",
    "- Itera sobre cada fila del DataFrame `data`.\n",
    "- Itera sobre las columnas de diagnóstico y actualiza el diccionario `diagnosticos` con la frecuencia de cada diagnóstico.\n",
    "- Itera sobre las columnas de procedimientos y actualiza el diccionario `procedimientos` con la frecuencia de cada procedimiento.\n",
    "- Crea listas de tokens únicos para diagnósticos y procedimientos.\n",
    "- Calcula el tamaño del vocabulario combinando el número de diagnósticos y procedimientos únicos, y le suma 1.\n",
    "- Imprime el número de diagnósticos, procedimientos y el tamaño del vocabulario.\n",
    "\n",
    "**7. Tokenización:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcefd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_rows=[]\n",
    "for i, row in data.iterrows():\n",
    "    token_row=[]\n",
    "    for j in range(1,36):\n",
    "        field=\"Diag\"+str(j).zfill(2)\n",
    "        token_row.append(1+token_diagnosticos.index(row[field]))\n",
    "    for j in range(1,31):\n",
    "        field=\"Proced\"+str(j).zfill(2)\n",
    "        token_row.append(1+377+token_procedimientos.index(row[field]))\n",
    "    token_rows.append(token_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9a229",
   "metadata": {},
   "source": [
    "- Inicializa una lista vacía para almacenar las filas tokenizadas.\n",
    "- Itera sobre cada fila del DataFrame `data`.\n",
    "- Itera sobre las columnas de diagnóstico y agrega el índice del diagnóstico en la lista `token_diagnosticos` (más 1) a la fila tokenizada.\n",
    "- Itera sobre las columnas de procedimientos y agrega el índice del procedimiento en la lista `token_procedimientos` (más 1 y más 377) a la fila tokenizada.\n",
    "- Agrega la fila tokenizada a la lista `token_rows`.\n",
    "\n",
    "En resumen, este código carga un archivo CSV, realiza una limpieza y transformación de los datos, crea un DataFrame de pandas, explora los datos mediante estadísticas y visualizaciones, filtra los datos según ciertos criterios, analiza los diagnósticos y procedimientos más frecuentes, y tokeniza los datos para su uso en modelos de aprendizaje automático."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
